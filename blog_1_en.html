<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Classification of Iris Data with XGBoost</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
        }
        h1, h2 {
            color: #333;
        }
        .footer {
            margin-top: 40px;
            font-style: italic;
            color: #555;
        }
    </style>
</head>
<body>
    <h1>Classification of Iris Data with XGBoost</h1>
    <p>In this blog post, we will demonstrate how to classify Iris flower data using the XGBoost algorithm. XGBoost is a popular and powerful machine learning algorithm known for its high performance and efficiency.</p>
    
    <h2>Data Preparation</h2>
    <p>We start by importing the necessary libraries and loading the Iris dataset. The Iris dataset is a classic dataset in machine learning and contains three classes of Iris flowers: Setosa, Versicolour, and Virginica.</p>
    <pre>
import xgboost as xgb
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load iris data
iris = load_iris()
X = iris.data
y = iris.target

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    </pre>

    <h2>Model Training</h2>
    <p>Next, we convert the data into DMatrix format, which is the data structure that XGBoost uses. We then define the parameters for the XGBoost model and train it using the training data.</p>
    <pre>
# Convert data into DMatrix format
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Define parameters
params = {
    'objective': 'multi:softmax',
    'num_class': 3,
    'eval_metric': 'mlogloss'
}

# Train the model
bst = xgb.train(params, dtrain, num_boost_round=100)
    </pre>

    <h2>Model Evaluation</h2>
    <p>After training the model, we use it to make predictions on the test data and evaluate its performance using accuracy as the metric.</p>
    <pre>
# Make predictions
y_pred = bst.predict(dtest)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")
    </pre>
    <p>The model achieved an accuracy of 96.67%, indicating that it performs very well on the Iris dataset.</p>
    
    <h2>Conclusion</h2>
    <p>Using XGBoost for classifying Iris data is highly effective, as demonstrated by the high accuracy achieved in this example. XGBoost's efficiency and performance make it a great choice for various machine learning tasks.</p>
    
    <div class="footer">
        <p>Reflecting on the model's performance, it is evident that well-constructed models like XGBoost can make highly accurate predictions, showcasing that determinations and predictions in machine learning are far from being mere coincidences. The combination of robust algorithms and quality data often leads to precise and reliable outcomes, highlighting the strength of machine learning in extracting meaningful insights from data.</p>
    </div>
</body>
</html>
